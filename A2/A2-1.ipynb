{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2: Entity Resolution (Part 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data scientists often spend 80% of their time on [data preparation](https://www.infoworld.com/article/3228245/the-80-20-data-science-dilemma.html). If your career goal is to become a data scientist, you have to master data cleaning and data integration skills. In this assignment, you will learn how to solve the Entity Resolution (ER) problem, a very common problem in data cleaning and integration. After completing this assignment, you should be able to answer the following questions:\n",
    "\n",
    "1. What is ER?\n",
    "2. What are the applications of ER in data integration and cleaning? \n",
    "3. How to avoid $n^2$ comparisons? \n",
    "4. How to compute Jaccard Similarity?\n",
    "5. How to evaluate an ER result?\n",
    "\n",
    "**Requirements:**\n",
    "\n",
    "1. Please use [pandas.DataFrame](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.html) rather than spark.DataFrame to manipulate data.\n",
    "\n",
    "2. Please follow python code style (https://www.python.org/dev/peps/pep-0008/). If TA finds your code hard to read, you will lose points. This requirement will stay for the whole semester.\n",
    "\n",
    "The data for Assignment 2 (Part 1 and Part 2) are in `A2-data.zip`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ER is defined as finding different records that refer to the same real-world entity, e.g., iPhone 4-th generation vs. iPhone four. It is central to data integration and cleaning. In this assignment, you will learn how to apply ER in a data integration setting. But the program that you are going to write can be easily extended to a data-cleaning setting, being used to detect _duplication values_.   \n",
    "\n",
    "Imagine that you want to help your company's customers to buy products at a cheaper price. In order to do so, you first write a [web scraper](https://nbviewer.jupyter.org/github/sfu-db/bigdata-cmpt733/blob/master/Assignments/A1/A1-1.ipynb) to crawl product data from Amazon.com and Google Shopping, respectively, and then integrate the data together. Since the same product may have different representations in the two websites, you are facing an ER problem. \n",
    "\n",
    "Existing ER techniques can be broadly divided into two categories: similarity-based (Part 1) and learning-based (Part 2). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similarity Join"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike a learning-based technique, a similarity-based technique (a.k.a similarity join) does not need any label data. It first chooses a similarity function and a threshold, and then returns the record pairs whose similarity values are above the threshold. These returned record pairs are thought of as matching pairs, i.e., referring to the same real-world entity. \n",
    "\n",
    "Depending on particular applications, you may need to choose different similarity functions. In this assignment, we will use Jaccard similarity, i.e., $\\textsf{Jaccard}(r, s) = \\big|\\frac{r \\cap s}{r \\cup s}\\big|$. Here is the formal definition of this problem.\n",
    "\n",
    "> **Jaccard-Similarity Join**: Given two DataFrames, R and S, and a threshold $\\theta \\in (0, 1]$, the jaccard-similarity join problem aims to find all record pairs $(r, s) \\in R \\times S$ such that $\\textsf{Jaccard}(r, s) \\geq \\theta$  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To implement similarity join, you need to address the following challenges:\n",
    "\n",
    "1. Jaccard is used to quantify the similarity between two sets instead of two records. You need to convert each record to a set.\n",
    "\n",
    "2. A naive implementation of similarity join is to compute Jaccard for all $|R \\times S|$ possible pairs. Imagine R and S have one million records. This requires doing 10^12 pair comparisons, which is extremely expensive. Thus, you need to know how to avoid n^2 comparisons. \n",
    "\n",
    "3. The output of ER is a set of matching pairs, where each pair is considered as referring to the same real-world entity. You need to know how to evaluate the quality of an ER result.\n",
    "\n",
    "Next, you will be guided to complete four tasks. After finishing these tasks, I suggest you going over the above challenges again, and understand how they are addressed.\n",
    "\n",
    "Read the code in `similarity_join.py` first, and then implement the remaining four functions: `preprocess_df`, `filtering`, `verification`, and `evaluate` by doing Tasks A-D, respectively.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The program will output the following when running on the sample data:\n",
    "\n",
    "\n",
    "> Before filtering: 256 pairs in total\n",
    "\n",
    "> After Filtering: 84 pairs left\n",
    "\n",
    "> After Verification: 6 similar pairs\n",
    "\n",
    "> (precision, recall, fmeasure) =  (1.0, 0.375, 0.5454545454545454)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task A. Data Preprocessing (Record --> Token Set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since Jaccard needs to take two sets as input, your first job is to preprocess DataFrames by transforming each record into a set of tokens. Please implement the following function.   \n",
    "\n",
    "```python\n",
    "def preprocess_df(self, df, cols): \n",
    "    \"\"\"\n",
    "    Input:\n",
    "        df: a pandas DataFrame\n",
    "        cols: a list of column names in df to be concatenated and tokenized\n",
    "\n",
    "    Output:\n",
    "        new_df: a new DataFrame that is the same as df, but with an extra column:\n",
    "                - joinKey: a list of tokens for each record\n",
    "    \"\"\"\n",
    "    \n",
    "    # Make a copy so we do not change the original dataframe\n",
    "    new_df = df.copy()\n",
    "\n",
    "    def build_joinkey(row):\n",
    "        # Join the selected columns into one string\n",
    "        concat_str = \" \".join(\n",
    "            str(row[c]) if pd.notna(row[c]) else \"\"\n",
    "            for c in cols\n",
    "        )\n",
    "\n",
    "        # Split the string into tokens and convert to lower-case\n",
    "        tokens = [t.lower() for t in re.split(r'\\W+', concat_str) if t != \"\"]\n",
    "\n",
    "        return tokens\n",
    "\n",
    "    # Apply the function to every row to create joinKey\n",
    "    new_df[\"joinKey\"] = new_df.apply(build_joinkey, axis=1)\n",
    "\n",
    "    return new_df\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>joinKey</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>b0002mg5uk</td>\n",
       "      <td>[iview, mediapro, 2, 5, global, marketing]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>b0002jtvng</td>\n",
       "      <td>[bias, deck, le, 3, 5, macintosh, cd, bias]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>b0007lw22g</td>\n",
       "      <td>[apple, ilife, 06, mac, dvd, older, version, a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>b00007kh02</td>\n",
       "      <td>[extensis, intellihance, pro, 4, x, win, mac, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>b000saufpw</td>\n",
       "      <td>[dk, amazing, animals, 1, 1, global, software,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           id                                            joinKey\n",
       "0  b0002mg5uk         [iview, mediapro, 2, 5, global, marketing]\n",
       "1  b0002jtvng        [bias, deck, le, 3, 5, macintosh, cd, bias]\n",
       "2  b0007lw22g  [apple, ilife, 06, mac, dvd, older, version, a...\n",
       "3  b00007kh02  [extensis, intellihance, pro, 4, x, win, mac, ...\n",
       "4  b000saufpw  [dk, amazing, animals, 1, 1, global, software,..."
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from similarity_join import SimilarityJoin\n",
    "\n",
    "er = SimilarityJoin(\"A2-data/part1-similarity-join/Amazon-Google-Sample/Amazon_sample.csv\", \"A2-data/part1-similarity-join/Amazon-Google-Sample/Google_sample.csv\")\n",
    "\n",
    "amazon_cols = [\"title\", \"manufacturer\"]\n",
    "google_cols = [\"name\", \"manufacturer\"]\n",
    "\n",
    "df1 = er.preprocess_df(er.df1, amazon_cols)\n",
    "df2 = er.preprocess_df(er.df2, google_cols)\n",
    "\n",
    "# joinKey가 잘 만들어졌는지 확인\n",
    "df1[[\"id\", \"joinKey\"]].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the purpose of testing, you can compare your outputs with new_df1 and new_df2 that can be found from the `Amazon-Google-Sample` folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task B. Filtering Obviously Non-matching Pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To avoid $n^2$ pair comparisons, ER algorithms often follow a filtering-and-verification framework. The basic idea is to first filter obviously non-matching pairs and then only verify the remaining pairs.  \n",
    "\n",
    "In Task B, your job is to implement the <font color=\"blue\">filtering</font> function. This function will filter all the record pairs whose joinKeys do not share any token. This is because based on the definition of Jaccard, we can deduce that **if two sets do not share any element (i.e., $r\\cap s = \\phi$), their Jaccard similarity values must be zero**. Thus, we can safely remove them. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "def filtering(self, df1, df2):\n",
    "    \"\"\"\n",
    "    Input: df1, df2 have a 'joinKey' column (list of tokens) and an 'id' column\n",
    "    Output: cand_df with columns: id1, joinKey1, id2, joinKey2\n",
    "            Keep only pairs that share at least one token\n",
    "    \"\"\"\n",
    "\n",
    "    # Build an inverted index for df1\n",
    "    # token -> list of row indices in df1\n",
    "    inv_index = {}\n",
    "\n",
    "    for i, tokens in enumerate(df1[\"joinKey\"]):\n",
    "        # use set() to avoid duplicate tokens in one record\n",
    "        for t in set(tokens):\n",
    "            if t not in inv_index:\n",
    "                inv_index[t] = []\n",
    "            inv_index[t].append(i)\n",
    "\n",
    "    # Find candidate pairs using df2\n",
    "    # use set to remove duplicate (i, j) pairs\n",
    "    cand_pairs = set()\n",
    "\n",
    "    for j, tokens2 in enumerate(df2[\"joinKey\"]):\n",
    "        for t in set(tokens2):\n",
    "            if t in inv_index:\n",
    "                for i in inv_index[t]:\n",
    "                    cand_pairs.add((i, j))\n",
    "\n",
    "    # Build candidate DataFrame\n",
    "    rows = []\n",
    "    \n",
    "    for i, j in cand_pairs:\n",
    "        rows.append({\n",
    "            \"id1\": df1.iloc[i][\"id\"],\n",
    "            \"joinKey1\": df1.iloc[i][\"joinKey\"],\n",
    "            \"id2\": df2.iloc[j][\"id\"],\n",
    "            \"joinKey2\": df2.iloc[j][\"joinKey\"]\n",
    "        })\n",
    "\n",
    "    cand_df = pd.DataFrame(rows, columns=[\"id1\", \"joinKey1\", \"id2\", \"joinKey2\"])\n",
    "\n",
    "    return cand_df\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id1</th>\n",
       "      <th>joinKey1</th>\n",
       "      <th>id2</th>\n",
       "      <th>joinKey2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>b000saufpw</td>\n",
       "      <td>[dk, amazing, animals, 1, 1, global, software,...</td>\n",
       "      <td>http://www.google.com/base/feeds/snippets/1102...</td>\n",
       "      <td>[punch, software, 20100, punch, 5, in, 1, home...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>b000saufpw</td>\n",
       "      <td>[dk, amazing, animals, 1, 1, global, software,...</td>\n",
       "      <td>http://www.google.com/base/feeds/snippets/1761...</td>\n",
       "      <td>[onone, software, essentials, for, adobe, phot...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>b000v7vz1u</td>\n",
       "      <td>[onone, essentials, for, adobe, photoshop, ele...</td>\n",
       "      <td>http://www.google.com/base/feeds/snippets/7249...</td>\n",
       "      <td>[onone, software, ice, 10770, intellihance, pr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>b000v7vz1u</td>\n",
       "      <td>[onone, essentials, for, adobe, photoshop, ele...</td>\n",
       "      <td>http://www.google.com/base/feeds/snippets/1020...</td>\n",
       "      <td>[palmspring, software, 523, oxford, american, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>b000jx1kma</td>\n",
       "      <td>[aircraft, power, pack, for, ms, flight, simul...</td>\n",
       "      <td>http://www.google.com/base/feeds/snippets/1761...</td>\n",
       "      <td>[onone, software, essentials, for, adobe, phot...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id1                                           joinKey1  \\\n",
       "0  b000saufpw  [dk, amazing, animals, 1, 1, global, software,...   \n",
       "1  b000saufpw  [dk, amazing, animals, 1, 1, global, software,...   \n",
       "2  b000v7vz1u  [onone, essentials, for, adobe, photoshop, ele...   \n",
       "3  b000v7vz1u  [onone, essentials, for, adobe, photoshop, ele...   \n",
       "4  b000jx1kma  [aircraft, power, pack, for, ms, flight, simul...   \n",
       "\n",
       "                                                 id2  \\\n",
       "0  http://www.google.com/base/feeds/snippets/1102...   \n",
       "1  http://www.google.com/base/feeds/snippets/1761...   \n",
       "2  http://www.google.com/base/feeds/snippets/7249...   \n",
       "3  http://www.google.com/base/feeds/snippets/1020...   \n",
       "4  http://www.google.com/base/feeds/snippets/1761...   \n",
       "\n",
       "                                            joinKey2  \n",
       "0  [punch, software, 20100, punch, 5, in, 1, home...  \n",
       "1  [onone, software, essentials, for, adobe, phot...  \n",
       "2  [onone, software, ice, 10770, intellihance, pr...  \n",
       "3  [palmspring, software, 523, oxford, american, ...  \n",
       "4  [onone, software, essentials, for, adobe, phot...  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from similarity_join import SimilarityJoin\n",
    "import pandas as pd\n",
    "\n",
    "er = SimilarityJoin(\n",
    "    \"A2-data/part1-similarity-join/Amazon-Google-Sample/Amazon_sample.csv\",\n",
    "    \"A2-data/part1-similarity-join/Amazon-Google-Sample/Google_sample.csv\"\n",
    ")\n",
    "\n",
    "amazon_cols = [\"title\", \"manufacturer\"]\n",
    "google_cols = [\"name\", \"manufacturer\"]\n",
    "\n",
    "df1 = er.preprocess_df(er.df1, amazon_cols)\n",
    "df2 = er.preprocess_df(er.df2, google_cols)\n",
    "\n",
    "cand_df = er.filtering(df1, df2)\n",
    "\n",
    "cand_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id1</th>\n",
       "      <th>joinKey1</th>\n",
       "      <th>id2</th>\n",
       "      <th>joinKey2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>b0002mg5uk</td>\n",
       "      <td>['iview', 'mediapro', '2', '5', 'global', 'mar...</td>\n",
       "      <td>http://www.google.com/base/feeds/snippets/1267...</td>\n",
       "      <td>['iview', 'mediapro', '2', '6', 'media', 'mana...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>b0002mg5uk</td>\n",
       "      <td>['iview', 'mediapro', '2', '5', 'global', 'mar...</td>\n",
       "      <td>http://www.google.com/base/feeds/snippets/1761...</td>\n",
       "      <td>['onone', 'software', 'essentials', 'for', 'ad...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>b000v7vz1u</td>\n",
       "      <td>['onone', 'essentials', 'for', 'adobe', 'photo...</td>\n",
       "      <td>http://www.google.com/base/feeds/snippets/1267...</td>\n",
       "      <td>['iview', 'mediapro', '2', '6', 'media', 'mana...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>b000v7vz1u</td>\n",
       "      <td>['onone', 'essentials', 'for', 'adobe', 'photo...</td>\n",
       "      <td>http://www.google.com/base/feeds/snippets/1761...</td>\n",
       "      <td>['onone', 'software', 'essentials', 'for', 'ad...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>b0002mg5uk</td>\n",
       "      <td>['iview', 'mediapro', '2', '5', 'global', 'mar...</td>\n",
       "      <td>http://www.google.com/base/feeds/snippets/1102...</td>\n",
       "      <td>['punch', 'software', '20100', 'punch', '5', '...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id1                                           joinKey1  \\\n",
       "0  b0002mg5uk  ['iview', 'mediapro', '2', '5', 'global', 'mar...   \n",
       "1  b0002mg5uk  ['iview', 'mediapro', '2', '5', 'global', 'mar...   \n",
       "2  b000v7vz1u  ['onone', 'essentials', 'for', 'adobe', 'photo...   \n",
       "3  b000v7vz1u  ['onone', 'essentials', 'for', 'adobe', 'photo...   \n",
       "4  b0002mg5uk  ['iview', 'mediapro', '2', '5', 'global', 'mar...   \n",
       "\n",
       "                                                 id2  \\\n",
       "0  http://www.google.com/base/feeds/snippets/1267...   \n",
       "1  http://www.google.com/base/feeds/snippets/1761...   \n",
       "2  http://www.google.com/base/feeds/snippets/1267...   \n",
       "3  http://www.google.com/base/feeds/snippets/1761...   \n",
       "4  http://www.google.com/base/feeds/snippets/1102...   \n",
       "\n",
       "                                            joinKey2  \n",
       "0  ['iview', 'mediapro', '2', '6', 'media', 'mana...  \n",
       "1  ['onone', 'software', 'essentials', 'for', 'ad...  \n",
       "2  ['iview', 'mediapro', '2', '6', 'media', 'mana...  \n",
       "3  ['onone', 'software', 'essentials', 'for', 'ad...  \n",
       "4  ['punch', 'software', '20100', 'punch', '5', '...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gt_cand = pd.read_csv(\n",
    "    \"A2-data/part1-similarity-join/Amazon-Google-Sample/cand_df.csv\"\n",
    ")\n",
    "\n",
    "gt_cand.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My cand_df: 84\n",
      "GT cand_df: 84\n"
     ]
    }
   ],
   "source": [
    "print(\"My cand_df:\", len(cand_df))\n",
    "print(\"GT cand_df:\", len(gt_cand))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the purpose of testing, you can compare your output with cand_df that can be found from the `Amazon-Google-Sample` folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task C. Computing Jaccard Similarity for Survived Pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the second phase of the filtering-and-verification framework, we will compute the Jaccard similarity for each survived pair and return those pairs whose jaccard similarity values are no smaller than the specified threshold.\n",
    "\n",
    "In Task C, your job is to implement the <font color=\"blue\">verification</font> function. This task looks simple, but there are a few small \"traps\". \n",
    "\n",
    "\n",
    "```python\n",
    "def verification(self, cand_df, threshold):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        cand_df: the output DataFrame from filtering()\n",
    "                 It has columns: id1, joinKey1, id2, joinKey2\n",
    "        threshold: float in (0, 1]\n",
    "\n",
    "    Output:\n",
    "        result_df: a new DataFrame with columns:\n",
    "                   id1, joinKey1, id2, joinKey2, jaccard\n",
    "                   Keep only rows with jaccard >= threshold\n",
    "    \"\"\"\n",
    "\n",
    "    rows = []\n",
    "\n",
    "    for _, row in cand_df.iterrows():\n",
    "        tokens1 = row[\"joinKey1\"]\n",
    "        tokens2 = row[\"joinKey2\"]\n",
    "\n",
    "        # Jaccard is defined on sets (remove duplicates)\n",
    "        set1 = set(tokens1)\n",
    "        set2 = set(tokens2)\n",
    "\n",
    "        inter = set1.intersection(set2)\n",
    "        union = set1.union(set2)\n",
    "\n",
    "        # avoid division by zero (just in case)\n",
    "        jacc = 0.0 if len(union) == 0 else len(inter) / len(union)\n",
    "\n",
    "        # keep only pairs whose jaccard is no smaller than threshold\n",
    "        if jacc >= threshold:\n",
    "            rows.append({\n",
    "                \"id1\": row[\"id1\"],\n",
    "                \"joinKey1\": row[\"joinKey1\"],\n",
    "                \"id2\": row[\"id2\"],\n",
    "                \"joinKey2\": row[\"joinKey2\"],\n",
    "                \"jaccard\": jacc\n",
    "            })\n",
    "\n",
    "    result_df = pd.DataFrame(rows, columns=[\"id1\", \"joinKey1\", \"id2\", \"joinKey2\", \"jaccard\"])\n",
    "    return result_df\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My result_df: 6\n",
      "GT result_df: 6\n"
     ]
    }
   ],
   "source": [
    "cand_df = er.filtering(df1, df2)\n",
    "result_df = er.verification(cand_df, 0.5)\n",
    "\n",
    "print(\"My result_df:\", len(result_df))\n",
    "\n",
    "gt_result = pd.read_csv(\"A2-data/part1-similarity-join/Amazon-Google-Sample/result_df.csv\")\n",
    "print(\"GT result_df:\", len(gt_result))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the purpose of testing, you can compare your output with result_df that can be found from the `Amazon-Google-Sample` folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task D. Evaluating an ER result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How should we evaluate an ER result? Before answering this question, let's first recall what the ER result looks like. The goal of ER is to identify all matching record pairs. Thus, the ER result should be a set of identified matching pairs, denoted by R. One thing that we want to know is that what percentage of the pairs in $R$ that are truly matching? This is what Precision can tell us. Let $T$ denote the truly matching pairs in $R$. Precision is defined as:\n",
    "$$Precision = \\frac{|T|}{|R|}$$\n",
    "\n",
    "In addition to Precision, another thing that we care about is that what percentage of truly matching pairs that are identified. This is what Recall can tell us. Let $A$ denote the truly matching pairs in the entire dataset. Recall is defined as: \n",
    "\n",
    "$$Recall = \\frac{|T|}{|A|}$$\n",
    "\n",
    "There is an interesting trade-off between Precision and Recall. As more and more pairs that are identified as matching, Recall increases while Precision potentially decreases. For the extreme case, if we return all the pairs as matching pairs, we will get a perfect Recall (i.e., Recall = 100%) but precision will be the worst. Thus, to balance Precision and Recall, people often use FMeasure to evaluate an ER result:\n",
    "\n",
    "$$FMeasure = \\frac{2*Precision*Recall}{Precision+Recall}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Task D, you will be given an ER result as well as the ground truth that tells you what pairs are truly matching. Your job is to calculate Precision, Recall and FMeasure for the result. \n",
    "```python\n",
    "def evaluate(self, result, ground_truth):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        result: a list of matching pairs found by your ER algorithm\n",
    "                e.g., [[\"a1\",\"g1\"], [\"a2\",\"g2\"], ...]\n",
    "        ground_truth: a list of true matching pairs (human-labeled)\n",
    "\n",
    "    Output:\n",
    "        (precision, recall, fmeasure)\n",
    "    \"\"\"\n",
    "\n",
    "    # Make them sets of tuples so we can compare easily\n",
    "    # (Also removes duplicates automatically)\n",
    "    result_set = set(tuple(p) for p in result)\n",
    "    gt_set = set(tuple(p) for p in ground_truth)\n",
    "\n",
    "    # True positives = pairs that appear in BOTH result and ground truth\n",
    "    true_positive = result_set.intersection(gt_set)\n",
    "\n",
    "    # Compute precision and recall (handle divide-by-zero)\n",
    "    precision = len(true_positive) / len(result_set) if len(result_set) > 0 else 0.0\n",
    "    recall = len(true_positive) / len(gt_set) if len(gt_set) > 0 else 0.0\n",
    "\n",
    "    # Compute F-measure (harmonic mean)\n",
    "    fmeasure = (2 * precision * recall / (precision + recall)) if (precision + recall) > 0 else 0.0\n",
    "\n",
    "    return (precision, recall, fmeasure)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "내가 찾은 매칭 수: 6\n",
      "정답 매칭 수: 16\n",
      "맞춘 매칭 수(True Positive): 6\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id1</th>\n",
       "      <th>id2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>b000licg1m</td>\n",
       "      <td>http://www.google.com/base/feeds/snippets/6070...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>b000v7vz1u</td>\n",
       "      <td>http://www.google.com/base/feeds/snippets/1761...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>b000ndibge</td>\n",
       "      <td>http://www.google.com/base/feeds/snippets/1227...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>b000jx1kma</td>\n",
       "      <td>http://www.google.com/base/feeds/snippets/1835...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>b00002s6sc</td>\n",
       "      <td>http://www.google.com/base/feeds/snippets/1102...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>b000ndibbo</td>\n",
       "      <td>http://www.google.com/base/feeds/snippets/1693...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id1                                                id2\n",
       "0  b000licg1m  http://www.google.com/base/feeds/snippets/6070...\n",
       "1  b000v7vz1u  http://www.google.com/base/feeds/snippets/1761...\n",
       "2  b000ndibge  http://www.google.com/base/feeds/snippets/1227...\n",
       "3  b000jx1kma  http://www.google.com/base/feeds/snippets/1835...\n",
       "4  b00002s6sc  http://www.google.com/base/feeds/snippets/1102...\n",
       "5  b000ndibbo  http://www.google.com/base/feeds/snippets/1693..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# ✅ 정답 매칭표 로드 (파일명이 다르면 그 파일명으로)\n",
    "gt = pd.read_csv(\"A2-data/part1-similarity-join/Amazon-Google-Sample/Amazon_Google_perfectMapping_sample.csv\")\n",
    "\n",
    "# (정답표 컬럼명이 idAmazon / idGoogle이면)\n",
    "gt_pairs = gt.rename(columns={\"idAmazon\":\"id1\", \"idGoogle\":\"id2\"})[[\"id1\",\"id2\"]]\n",
    "\n",
    "# ✅ 내 결과 (result_df가 이미 있다고 가정)\n",
    "my_pairs = result_df[[\"id1\",\"id2\"]].copy()\n",
    "\n",
    "# ✅ 내가 맞춘 것(교집합)\n",
    "hit = my_pairs.merge(gt_pairs, on=[\"id1\",\"id2\"], how=\"inner\")\n",
    "\n",
    "print(\"내가 찾은 매칭 수:\", len(my_pairs))\n",
    "print(\"정답 매칭 수:\", len(gt_pairs))\n",
    "print(\"맞춘 매칭 수(True Positive):\", len(hit))\n",
    "\n",
    "hit.head(20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "놓친 정답 수: 10\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id1</th>\n",
       "      <th>id2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>b0002mg5uk</td>\n",
       "      <td>http://www.google.com/base/feeds/snippets/1267...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>b0002jtvng</td>\n",
       "      <td>http://www.google.com/base/feeds/snippets/5572...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>b00007kh02</td>\n",
       "      <td>http://www.google.com/base/feeds/snippets/7249...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>b00006hvvo</td>\n",
       "      <td>http://www.google.com/base/feeds/snippets/3409...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>b0007lw22g</td>\n",
       "      <td>http://www.google.com/base/feeds/snippets/1084...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>b000saufpw</td>\n",
       "      <td>http://www.google.com/base/feeds/snippets/1007...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>b000in8n30</td>\n",
       "      <td>http://www.google.com/base/feeds/snippets/4377...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>b0000vyk1o</td>\n",
       "      <td>http://www.google.com/base/feeds/snippets/1740...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>b000gaqlxe</td>\n",
       "      <td>http://www.google.com/base/feeds/snippets/1479...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>b00013wh0w</td>\n",
       "      <td>http://www.google.com/base/feeds/snippets/1020...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           id1                                                id2\n",
       "0   b0002mg5uk  http://www.google.com/base/feeds/snippets/1267...\n",
       "1   b0002jtvng  http://www.google.com/base/feeds/snippets/5572...\n",
       "2   b00007kh02  http://www.google.com/base/feeds/snippets/7249...\n",
       "3   b00006hvvo  http://www.google.com/base/feeds/snippets/3409...\n",
       "4   b0007lw22g  http://www.google.com/base/feeds/snippets/1084...\n",
       "5   b000saufpw  http://www.google.com/base/feeds/snippets/1007...\n",
       "10  b000in8n30  http://www.google.com/base/feeds/snippets/4377...\n",
       "11  b0000vyk1o  http://www.google.com/base/feeds/snippets/1740...\n",
       "13  b000gaqlxe  http://www.google.com/base/feeds/snippets/1479...\n",
       "14  b00013wh0w  http://www.google.com/base/feeds/snippets/1020..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "missed = gt_pairs.merge(my_pairs, on=[\"id1\",\"id2\"], how=\"left\", indicator=True)\n",
    "missed = missed[missed[\"_merge\"]==\"left_only\"].drop(columns=[\"_merge\"])\n",
    "\n",
    "print(\"놓친 정답 수:\", len(missed))\n",
    "missed.head(20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement `preprocess_df`, `filtering`, `verification`, and `evaluate` functions in `similarity_join.py`. Submit your code file (`similarity_join.py`) to Canvas."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (dataprep310)",
   "language": "python",
   "name": "dataprep310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  },
  "vscode": {
   "interpreter": {
    "hash": "1a1af0ee75eeea9e2e1ee996c87e7a2b11a0bebd85af04bb136d915cefc0abce"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
